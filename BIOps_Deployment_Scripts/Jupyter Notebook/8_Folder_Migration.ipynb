{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>QuickSight Folder Migration</strong>\n",
    "\n",
    "Author: Ian Liao (Data Visualization Engineer in ProServe GSP)\n",
    "Date: Sep 13 2021\n",
    "\n",
    "QuickSight Folder can be used to build deployment environments. User can create DEV, UAT and PROD folders, and use them as corresponding environments.\n",
    "This migration script helps to dashboards with dependencies from one folder to another. \n",
    "Folders can be in the same QuickSight account, or different QuickSight accounts.\n",
    "\n",
    "<strong>Requirements</strong>\n",
    "\n",
    "The Dashboard name has to be unique globally</br>\n",
    "Folder name has be to unique globally. </br>\n",
    "Always use CAPITAL LETTERS ONLY NO NUMBERS NO SPECIAL CHARACTERS for folder name. </br>\n",
    "A DEV folder has to exist, and it is used as the base folder (or referred as main). </br>\n",
    "Never migrate backwards, start a new development folder for hot-fix if necessary. Migrate to development folder creates an analysis instead of a dashboard </br> \n",
    "DO NOT use dash in dataset name </br>\n",
    "\n",
    "<strong>How it works?</strong>\n",
    "\n",
    "The script will try to find the specified dashboards in the source folder, find their datasets, and migrate to destination folder. </br>\n",
    "Object migrated to any folder other than DEV will use \"its ID in DEV - folder name\" as the new ID. </br>\n",
    "\n",
    "<strong>What objects are migrated</strong>\n",
    "\n",
    "In same account migration, Datasets and Dashboards are migrated; </br>\n",
    "In cross account migration, Datasets, Themes and Dashboards are migrated. </br>\n",
    "In either case, data source is not migrated. It assumes only one data source, and user should provide the datasource id in the target account\n",
    "\n",
    "<strong>Limitations</strong>\n",
    "\n",
    "Dataset refresh schedule is not set in target environment </br>\n",
    "Migrate from HF to UAT will create a new dashboard. Developer needs to delete or remove the older version dashboard manually.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install --upgrade boto3\n",
    "get_ipython().system('pip install --upgrade ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#DEV -> UAT\n",
    "# Configure the migration scripts\n",
    "\n",
    "# dashboard migration list\n",
    "dashboard_migrate_list = ['<name of dashboards to migrate>']\n",
    "theme_migrate_list = [] # provide theme name for cross account migration\n",
    "\n",
    "# source info\n",
    "sourceaccountid='<source_account_name>'\n",
    "source_role_name='<execution role for the source account>'\n",
    "source_aws_region='us-east-1'\n",
    "source_folder_name='DEV'\n",
    "source_folder_ID='<ID of the DEV folder>'\n",
    "source_is_dev = True # analysis exists in dev folder\n",
    "source_is_main = True # in main environment, object IDs don't have folder name suffix\n",
    "source_admin_name = '<name of QuickSight admin user>'\n",
    "\n",
    "\n",
    "# target info\n",
    "targetaccountid='<target_account_name>'\n",
    "target_role_name='<execution role for the account>'\n",
    "target_aws_region='us-east-1'\n",
    "target_folder_name='UAT'\n",
    "target_folder_ID='ID of the UAT folder'\n",
    "target_is_dev = False # set this flag to true if analysis migration is necessary (in use case like roll back); by default analysis will not be migrated. \n",
    "target_is_main = False\n",
    "target_admin_name = '<name of QuickSight admin user>'\n",
    "\n",
    "target_datasource_id = '' # provide target data source id for cross account migration\n",
    "\n",
    "# migration settings\n",
    "same_account_migration = (True if sourceaccountid == targetaccountid else False)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# UAT -> HF\n",
    "\n",
    "\n",
    "# Configure the migration scripts\n",
    "\n",
    "# dashboard migration list\n",
    "dashboard_migrate_list = ['<name of dashboards to migrate>'] # name of the dashboard\n",
    "theme_migrate_list = [] # provide theme name for cross account migration\n",
    "\n",
    "# source info\n",
    "sourceaccountid='<source_account_name>'\n",
    "source_role_name='<execution role for the source account>'\n",
    "source_aws_region='us-east-1'\n",
    "source_folder_name='UAT'\n",
    "source_folder_ID='<ID of the UAT folder>'\n",
    "source_is_dev = False # analysis exists in dev folder\n",
    "source_is_main = False # in main environment, object IDs don't have folder name suffix\n",
    "source_admin_name = '<name of QuickSight admin user>'\n",
    "\n",
    "\n",
    "# target info\n",
    "targetaccountid='233081732471'\n",
    "target_role_name='<execution role for the account>'\n",
    "target_aws_region='us-east-1'\n",
    "target_folder_name='HF'\n",
    "target_folder_ID='<ID of the HF folder>'\n",
    "target_is_dev = True # set this flag to true if analysis migration is necessary (in use case like roll back); by default analysis will not be migrated. \n",
    "target_is_main = False\n",
    "target_admin_name = '<name of QuickSight admin user>'\n",
    "\n",
    "target_datasource_id = '' # provide target data source id for cross account migration\n",
    "\n",
    "# migration settings\n",
    "same_account_migration = (True if sourceaccountid == targetaccountid else False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#HF -> UAT\n",
    "# Configure the migration scripts\n",
    "\n",
    "# dashboard migration list\n",
    "dashboard_migrate_list = ['<name of dashboards to migrate>']\n",
    "theme_migrate_list = [] # provide theme name for cross account migration\n",
    "\n",
    "# source info\n",
    "sourceaccountid='<source_account_name>'\n",
    "source_role_name='<execution role for the source account>'\n",
    "source_aws_region='us-east-1'\n",
    "source_folder_name='HF'\n",
    "source_folder_ID='<ID of the HF folder>'\n",
    "source_is_dev = True # analysis exists in dev folder\n",
    "source_is_main = False # in main environment, object IDs don't have folder name suffix\n",
    "source_admin_name = '<name of QuickSight admin user>'\n",
    "\n",
    "\n",
    "# target info\n",
    "targetaccountid='233081732471'\n",
    "target_role_name='<execution role for the account>'\n",
    "target_aws_region='us-east-1'\n",
    "target_folder_name='UAT'\n",
    "target_folder_ID='<ID of the UAT folder>'\n",
    "target_is_dev = False # set this flag to true if analysis migration is necessary (in use case like roll back); by default analysis will not be migrated. \n",
    "target_is_main = False\n",
    "target_admin_name = '<name of QuickSight admin user>'\n",
    "\n",
    "target_datasource_id = '' # provide target data source id for cross account migration\n",
    "\n",
    "# migration settings\n",
    "same_account_migration = (True if sourceaccountid == targetaccountid else False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''#UAT -> DEV\n",
    "# a rare use case to reset main branch\n",
    "# Don't run it unless absolute necessary\n",
    "# Configure the migration scripts\n",
    "\n",
    "\n",
    "# dashboard migration list\n",
    "dashboard_migrate_list = ['<name of dashboards to migrate>'] # name of the dashboard\n",
    "theme_migrate_list = [] # provide theme name for cross account migration\n",
    "\n",
    "# source info\n",
    "sourceaccountid='<source_account_name>'\n",
    "source_role_name='<execution role for the source account>'\n",
    "source_aws_region='us-east-1'\n",
    "source_folder_name='UAT'\n",
    "source_folder_ID='<ID of the UAT folder>'\n",
    "source_is_dev = False # analysis exists in dev folder\n",
    "source_is_main = False # in main environment, object IDs don't have folder name suffix\n",
    "source_admin_name = '<name of QuickSight admin user>'\n",
    "\n",
    "\n",
    "# target info\n",
    "targetaccountid='233081732471'\n",
    "target_role_name='<execution role for the account>'\n",
    "target_aws_region='us-east-1'\n",
    "target_folder_name='DEV'\n",
    "target_folder_ID='<ID of the DEV folder>'\n",
    "target_is_dev = True # set this flag to true if analysis migration is necessary (in use case like roll back); by default analysis will not be migrated. \n",
    "target_is_main = True # a rare use case to reset main branch\n",
    "target_admin_name = '<name of QuickSight admin user>'\n",
    "\n",
    "target_datasource_id = '' # provide target data source id for cross account migration\n",
    "\n",
    "# migration settings\n",
    "same_account_migration = (True if sourceaccountid == targetaccountid else False)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# create config dict\n",
    "config = {}\n",
    "config['dashboard_migrate_list'] = dashboard_migrate_list\n",
    "# source info\n",
    "config['sourceaccountid'] = sourceaccountid\n",
    "config['source_role_name']= source_role_name\n",
    "config['source_aws_region']= source_aws_region\n",
    "config['source_folder_name']= source_folder_name\n",
    "config['source_folder_ID']= source_folder_ID\n",
    "config['source_is_dev'] = source_is_dev\n",
    "config['source_is_main'] = source_is_main\n",
    "config['source_admin_name'] = source_admin_name\n",
    "\n",
    "\n",
    "# target info\n",
    "config['targetaccountid']=targetaccountid\n",
    "config['target_role_name']=target_role_name\n",
    "config['target_aws_region']=target_aws_region\n",
    "config['target_folder_name']=target_folder_name\n",
    "config['target_folder_ID']=target_folder_ID\n",
    "config['target_is_dev'] = target_is_dev\n",
    "config['target_admin_name'] = target_admin_name\n",
    "\n",
    "# migration settings\n",
    "config['same_account_migration'] = same_account_migration'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import time\n",
    "from IPython.display import JSON\n",
    "import sys\n",
    "import ipynb.fs \n",
    "import logging\n",
    "from typing import Any, Dict, List, Optional\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# current date and time\n",
    "now = str(datetime.now().strftime(\"%m-%d-%Y_%H_%M\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import functions from functions notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.defs.Functions import data_sources\n",
    "from ipynb.fs.defs.Functions import describe_source \n",
    "from ipynb.fs.defs.Functions import delete_source\n",
    "from ipynb.fs.defs.Functions import create_data_source\n",
    "from ipynb.fs.defs.Functions import get_datasource_name\n",
    "from ipynb.fs.defs.Functions import get_datasource_ids\n",
    "from ipynb.fs.defs.Functions import update_data_source_permissions\n",
    "\n",
    "from ipynb.fs.defs.Functions import get_dataset_name\n",
    "from ipynb.fs.defs.Functions import data_sets\n",
    "from ipynb.fs.defs.Functions import describe_dataset\n",
    "from ipynb.fs.defs.Functions import get_dataset_ids\n",
    "from ipynb.fs.defs.Functions import delete_dataset \n",
    "from ipynb.fs.defs.Functions import create_dataset\n",
    "from ipynb.fs.defs.Functions import update_dataset\n",
    "from ipynb.fs.defs.Functions import update_data_set_permissions\n",
    "\n",
    "from ipynb.fs.defs.Functions import get_target\n",
    "\n",
    "from ipynb.fs.defs.Functions import templates\n",
    "from ipynb.fs.defs.Functions import delete_template\n",
    "from ipynb.fs.defs.Functions import update_template_permission \n",
    "from ipynb.fs.defs.Functions import copy_template\n",
    "from ipynb.fs.defs.Functions import describe_template\n",
    "from ipynb.fs.defs.Functions import create_template \n",
    "\n",
    "from ipynb.fs.defs.Functions import dashboards\n",
    "from ipynb.fs.defs.Functions import describe_dashboard\n",
    "from ipynb.fs.defs.Functions import create_dashboard \n",
    "from ipynb.fs.defs.Functions import delete_dashboard\n",
    "from ipynb.fs.defs.Functions import update_dashboard \n",
    "from ipynb.fs.defs.Functions import get_dashboard_ids\n",
    "from ipynb.fs.defs.Functions import get_dashboard_name\n",
    "\n",
    "from ipynb.fs.defs.Functions import themes\n",
    "from ipynb.fs.defs.Functions import describe_theme\n",
    "from ipynb.fs.defs.Functions import delete_theme\n",
    "from ipynb.fs.defs.Functions import create_theme\n",
    "from ipynb.fs.defs.Functions import update_theme\n",
    "from ipynb.fs.defs.Functions import describe_theme_permissions\n",
    "from ipynb.fs.defs.Functions import update_theme_permissions\n",
    "\n",
    "from ipynb.fs.defs.Functions import analysis\n",
    "from ipynb.fs.defs.Functions import describe_analysis\n",
    "from ipynb.fs.defs.Functions import create_analysis\n",
    "from ipynb.fs.defs.Functions import delete_analysis\n",
    "from ipynb.fs.defs.Functions import update_analysis\n",
    "from ipynb.fs.defs.Functions import get_analysis_ids\n",
    "from ipynb.fs.defs.Functions import describe_analysis_permissions\n",
    "\n",
    "\n",
    "from ipynb.fs.defs.Functions import folder_members\n",
    "\n",
    "#supportive functions\n",
    "from ipynb.fs.defs.Functions import data_sets_ls_of_dashboard\n",
    "from ipynb.fs.defs.Functions import data_sources_ls_of_dashboard\n",
    "from ipynb.fs.defs.Functions import get_data_source_migration_list\n",
    "from ipynb.fs.defs.Functions import data_sources_ls_of_analysis\n",
    "from ipynb.fs.defs.Functions import data_sets_ls_of_analysis\n",
    "from ipynb.fs.defs.Functions import get_user_arn\n",
    "from ipynb.fs.defs.Functions import _assume_role\n",
    "\n",
    "from ipynb.fs.defs.Functions import create_folder_membership"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Static Profile</strong>\n",
    "\n",
    "You can also configure AWS profile from terminal and call the profile in below cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "sourceprofile=''\n",
    "targetprofile=''\n",
    "sourcesession = boto3.Session(profile_name=sourceprofile, region_name=source_aws_region)\n",
    "targetsession = boto3.Session(profile_name=targetprofile, region_name=target_aws_region)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sourceprofile=''\n",
    "targetprofile=''\n",
    "sourcesession = boto3.Session(region_name=source_aws_region)\n",
    "targetsession = boto3.Session(region_name=target_aws_region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Assume Role</strong>\n",
    "\n",
    "You can also assume an IAM role and create session based on the role permissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''#source account\n",
    "sourcesession = _assume_role(sourceaccountid, source_role_name, source_aws_region)\n",
    "\n",
    "#target account\n",
    "targetsession = _assume_role(targetaccountid, target_role_name, target_aws_region)\n",
    "#targetsession = boto3.Session(\n",
    "#        aws_access_key_id=\"\",\n",
    "#        aws_secret_access_key=\"\",\n",
    "#        aws_session_token=\"\",\n",
    "#        region_name=aws_region\n",
    "#    )'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Set root and admin users</strong>\n",
    "\n",
    "root user is for the template. \n",
    "By default, we assign full permissions of objects to admin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sourceroot=get_user_arn (sourcesession, 'root')\n",
    "sourceadmin=get_user_arn (sourcesession, source_admin_name)\n",
    "#sourceversion='1'\n",
    "\n",
    "targetroot=get_user_arn (targetsession, 'root')\n",
    "targetadmin=get_user_arn (targetsession, target_admin_name)\n",
    "#targetvpc='arn:aws:quicksight:us-east-1:889399602426:vpcConnection/sg-40b7521a'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Please define your input parameters in below cell</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data source credentials. May use in secret manager in future. \n",
    "\n",
    "rds='mssql'\n",
    "redshift={\n",
    "    \"ClusterId\": 'wangzyncluster1',\n",
    "    \"Host\": 'wangzyncluster1.coprq8ycemvc.us-east-1.redshift.amazonaws.com',\n",
    "    \"Database\": 'dev'}\n",
    "\n",
    "s3Bucket='spaceneedle-samplefiles.prod.us-east-1'\n",
    "s3Key='sales/manifest.json'\n",
    "vpc='sg-40b7521a'\n",
    "tag=[\n",
    "        {\n",
    "            'Key': 'covid-19-dashboard-migration',\n",
    "            'Value': 'true'\n",
    "        }\n",
    "    ]\n",
    "owner=targetadmin\n",
    "rdscredential={\n",
    "        'CredentialPair': {\n",
    "            'Username': \"\",\n",
    "            'Password': \"\"}}\n",
    "redshiftcredential={\n",
    "        'CredentialPair': {\n",
    "            'Username': \"ro_user\",\n",
    "            'Password': \"Ro_user1234\"}}\n",
    "region='us-east-1'\n",
    "namespace='default'\n",
    "version='1' \n",
    "\n",
    "target=get_target(targetsession, rds,redshift,s3Bucket,s3Key,vpc,tag,owner,rdscredential,redshiftcredential)\n",
    "\n",
    "JSON(target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Migration List<strong/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if same_account_migration == True:\n",
    "    migrate_p = 'dashboard'\n",
    "    source_migrate_list = []\n",
    "    theme_migrate_list= []\n",
    "else:\n",
    "    migrate_p = 'dashboard'\n",
    "    source_migrate_list = []\n",
    "    \n",
    " \n",
    "\"\"\"\"\n",
    "\"all\" will migrate data source, dataset, theme, analysis and dashboard;\n",
    "\"source\" means data sources only; \n",
    "\"dataset\" means datasets only; \n",
    "\"theme\" means theme only;\n",
    "\"analysis\" means analysis only;\n",
    "\"dashboard\" means dashboard only\n",
    "\"\"\" \n",
    "\n",
    "dataset_migrate_list = []\n",
    "analysis_migrate_list= []\n",
    "# dashboard migration list is now defined on top\n",
    "#dashboard_migrate_list = ['patient'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if migrate_p in ['dashboard']:\n",
    "    source_migrate_list=[]\n",
    "    dataset_migrate_list=[]\n",
    "    for dashboard in dashboard_migrate_list:\n",
    "        print(dashboard)\n",
    "        #datasources=data_sources_ls_of_dashboard(dashboard, sourcesession)\n",
    "        #print(datasources)\n",
    "        #for datasource in datasources:\n",
    "        #    source_migrate_list.append(datasource)\n",
    "        datasets=data_sets_ls_of_dashboard(dashboard, sourcesession)\n",
    "        for dataset in datasets:\n",
    "            dataset_migrate_list.append(dataset)\n",
    "            \n",
    "if migrate_p in ['analysis']:\n",
    "    source_migrate_list=[]\n",
    "    dataset_migrate_list=[]\n",
    "    for analysis_name in analysis_migrate_list:\n",
    "        print(analysis_name)\n",
    "        datasources=data_sources_ls_of_analysis(analysis_name, sourcesession)\n",
    "        print(datasources)\n",
    "        for datasource in datasources:\n",
    "            source_migrate_list.append(datasource)\n",
    "        datasets=data_sets_ls_of_analysis(analysis_name, sourcesession)\n",
    "        print(datasets)\n",
    "        for dataset in datasets:\n",
    "            dataset_migrate_list.append(dataset)\n",
    "    \n",
    "if migrate_p in ['all']:\n",
    "    for dashboard in dashboard_migrate_list:\n",
    "        datasources=data_sources_ls_of_dashboard(dashboard, sourcesession)\n",
    "        for datasource in datasources:\n",
    "            source_migrate_list.append(datasource)\n",
    "        datasets=data_sets_ls_of_dashboard(dashboard, sourcesession)\n",
    "        for dataset in datasets:\n",
    "            dataset_migrate_list.append(dataset)\n",
    "            \n",
    "    for analysis_name in analysis_migrate_list:\n",
    "        datasources=data_sources_ls_of_analysis(analysis_name, sourcesession)\n",
    "        for datasource in datasources:\n",
    "            source_migrate_list.append(datasource)\n",
    "        datasets=data_sets_ls_of_analysis(analysis_name, sourcesession)\n",
    "        for dataset in datasets:\n",
    "            dataset_migrate_list.append(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dashboard_migrate_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_migrate_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_migrate_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theme_migrate_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_migrate_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_id(source_id):\n",
    "    if source_is_main:\n",
    "        target_id = source_id + '-' + target_folder_name\n",
    "    elif target_is_main:\n",
    "        target_id = source_id.replace(('-' + source_folder_name), '')\n",
    "    else:\n",
    "        target_id = source_id.replace(('-' + source_folder_name), '') + ('-' + target_folder_name)\n",
    "        \n",
    "    return target_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_placeholder(placeholder):\n",
    "    if target_is_main:\n",
    "        target_placeholder = re.sub(r'-[A-Z]+', '', placeholder)\n",
    "    else:\n",
    "        target_placeholder = re.sub(r'-[A-Z]+', '', placeholder) + '-' + target_folder_name\n",
    "    return target_placeholder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Results Output Location<strong/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "successlocation = \"Migration_Results/Successful/\"\n",
    "faillocation = \"Migration_Results/Fail/\"\n",
    "\n",
    "import os\n",
    "try:\n",
    "    os.makedirs(successlocation)\n",
    "except OSError:\n",
    "    print (\"Creation of the directory %s failed\" % successlocation)\n",
    "else:\n",
    "    print (\"Successfully created the directory %s\" % successlocation)\n",
    "\n",
    "try:\n",
    "    os.makedirs(faillocation)\n",
    "except OSError:\n",
    "    print (\"Creation of the directory %s failed\" % faillocation)\n",
    "else:\n",
    "    print (\"Successfully created the directory %s\" % faillocation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Data Set Migration </strong> Get datasets list:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#source account\n",
    "sourceaccountid=\"\"\n",
    "role_name=\"\"\n",
    "aws_region=''\n",
    "sourcesession = _assume_role(sourceaccountid, role_name, aws_region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#target account\n",
    "targetaccountid=\"\"\n",
    "role_name=\"\"\n",
    "aws_region='us-east-1'\n",
    "targetsession = _assume_role(targetaccountid, role_name, aws_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets=data_sets(sourcesession)\n",
    "\n",
    "migration_list=[]\n",
    "for newset in dataset_migrate_list:\n",
    "        ids = get_dataset_ids(newset, sourcesession)  #Get id of datasets migration list\n",
    "        for dataset in datasets:\n",
    "            if ids[0] == dataset[\"DataSetId\"]:\n",
    "                migration_list.append(dataset)\n",
    "\n",
    "JSON(migration_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "migration_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Get already migrated datasets list</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get datasets which already migrated\n",
    "targetds=data_sets(targetsession)\n",
    "#already_migrated record the datasets ids of target account\n",
    "already_migrated=[]\n",
    "for ds in targetds:\n",
    "    already_migrated.append(ds['DataSetId'])\n",
    "#already_migrated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Migrate Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsetslist=[]\n",
    "faillist=[]\n",
    "sts_client = targetsession.client(\"sts\")\n",
    "account_id = sts_client.get_caller_identity()[\"Account\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def migrate_dataset(mds):  \n",
    "    print(get_target_id(mds['DataSetId']))\n",
    "    try:\n",
    "        res=describe_dataset(sourcesession, mds['DataSetId'])\n",
    "    except Exception:\n",
    "        faillist.append({\"Dataset\": mds, \"Error\": str(Exception)})\n",
    "        return\n",
    "    \n",
    "    if 'RowLevelPermissionDataSet' in res['DataSet']:\n",
    "        rls_flag = True\n",
    "        rls_arn = res['DataSet']['RowLevelPermissionDataSet']['Arn']\n",
    "        print('rls data set exists: ' + rls_arn)\n",
    "        rls_id = rls_arn.split('/')[1]\n",
    "        \n",
    "        try:\n",
    "            rls_res = describe_dataset(sourcesession, rls_id)\n",
    "        except Exception:\n",
    "            faillist.append({\"Dataset\": mds, \"RLS\": rls_id, \"Error\": str(Exception)})\n",
    "            return\n",
    "            \n",
    "        rls_name = rls_res['DataSet']['Name']\n",
    "        \n",
    "        rls = {}\n",
    "        rls['DataSetId'] = rls_id\n",
    "        rls['Name'] = rls_name\n",
    "        \n",
    "        print(rls)\n",
    "        #migrate the rls dataset to target\n",
    "        migrate_dataset(rls)\n",
    "        already_migrated.append(get_target_id(rls_id))\n",
    "        \n",
    "        RowLevelPermissionDataSet = res['DataSet']['RowLevelPermissionDataSet']\n",
    "        RowLevelPermissionDataSet['Arn'] = get_target_id(RowLevelPermissionDataSet['Arn'])\n",
    "        RowLevelPermissionDataSet['Status'] = 'ENABLED'\n",
    "        \n",
    "    else:\n",
    "        rls_flag = False\n",
    "        RowLevelPermissionDataSet = None\n",
    "        \n",
    "    \n",
    "    \n",
    "    name=mds['Name']\n",
    "    datasetid=mds['DataSetId']\n",
    "    \n",
    "    PT=res['DataSet']['PhysicalTableMap']\n",
    "    for key, value in PT.items():\n",
    "        for k,v in value.items():\n",
    "            dsid = v['DataSourceArn'].split(\"/\")[1] if same_account_migration else target_datasource_id\n",
    "            v['DataSourceArn']='arn:aws:quicksight:us-east-1:'+account_id+':datasource/'+dsid\n",
    "\n",
    "    LT=res['DataSet']['LogicalTableMap']\n",
    "    if 'ColumnGroups' in res['DataSet']:\n",
    "        ColumnGroups=res['DataSet']['ColumnGroups']\n",
    "    else: ColumnGroups=None\n",
    "    \n",
    "    if get_target_id(mds['DataSetId']) not in already_migrated:\n",
    "        try: \n",
    "            newdataset=create_dataset(targetsession, get_target_id(datasetid), get_target_id(name), PT, LT, res['DataSet']['ImportMode'], target['datasetpermission'],ColumnGroups, RowLevelPermissionDataSet)\n",
    "            print(\"new dataset: \", newdataset)\n",
    "            newsetslist.append(newdataset)\n",
    "        except Exception as e:\n",
    "            print('failed: '+str(e))\n",
    "            faillist.append({\"DataSetId\": datasetid, \"Name\": name, \"Error\": str(e)})\n",
    "            return\n",
    "            \n",
    "    if get_target_id(mds['DataSetId']) in already_migrated:\n",
    "        try: \n",
    "            newdataset=update_dataset(targetsession, get_target_id(datasetid), get_target_id(name), PT, LT, res['DataSet']['ImportMode'],ColumnGroups, RowLevelPermissionDataSet)\n",
    "            print(\"update dataset: \", newdataset)\n",
    "            newsetslist.append(newdataset)\n",
    "        except Exception as e:\n",
    "            print('failed: '+str(e))\n",
    "            faillist.append({\"DataSetId\": datasetid, \"Name\": name, \"Error\": str(e)})\n",
    "            return\n",
    "\n",
    "    create_folder_membership(targetsession, target_folder_ID, get_target_id(datasetid), 'DATASET' )\n",
    "    print('added DATASET {} to target folder'.format(get_target_id(datasetid)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "migration_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mds in migration_list:\n",
    "    migrate_dataset(mds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsetslist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faillist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print fail informations\n",
    "with open(faillocation+now+'Dataset_Creation_Error.json', \"w\") as f:\n",
    "            json.dump(faillist, f, indent=4, sort_keys=True, default=str)\n",
    "\n",
    "successfulls=[]\n",
    "for news in newsetslist:\n",
    "    dataset=describe_dataset(targetsession, news['DataSetId'])\n",
    "    successfulls.append(dataset['DataSet'])\n",
    "    \n",
    "with open(successlocation+now+'Datasets_Creation_Success.json', \"w\") as f:\n",
    "    json.dump(successfulls, f, indent=4, sort_keys=True, default=str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get themes list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "themes_list_complete =themes(sourcesession)\n",
    "themes_list=[]\n",
    "#JSON(datasets)\n",
    "for th in themes_list_complete:\n",
    "    if th[\"Name\"] in theme_migrate_list:\n",
    "        themes_list.append(th)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Migrate Themes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get themes which already migrated\n",
    "targetthemes=themes(targetsession)\n",
    "#already_migrated record the datasets ids of target account\n",
    "already_migrated=[]\n",
    "for th in targetthemes:\n",
    "    already_migrated.append(th['ThemeId'])\n",
    "#already_migrated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newthemeslist=[]\n",
    "faillist=[]\n",
    "sts_client = targetsession.client(\"sts\")\n",
    "account_id = sts_client.get_caller_identity()[\"Account\"]\n",
    "for i in themes_list:\n",
    "    if i['ThemeId'] not in already_migrated:\n",
    "        try:\n",
    "            res=describe_theme(sourcesession, i['ThemeId'])\n",
    "        except Exception:\n",
    "            faillist.append({\"Theme\": i, \"Error\": str(Exception)})\n",
    "            continue\n",
    "        THEMEID=res['Theme']['ThemeId']\n",
    "        Name=res['Theme']['Name']\n",
    "        BaseThemeId=res['Theme']['Version']['BaseThemeId']\n",
    "        Configuration=res['Theme']['Version']['Configuration']\n",
    "        try: \n",
    "            newtheme=create_theme (targetsession,THEMEID, Name,BaseThemeId,Configuration)\n",
    "            newthemeslist.append(newtheme)\n",
    "        except Exception as e:\n",
    "            #print('failed: '+str(e))\n",
    "            faillist.append({\"ThemeID\": THEMEID, \"Name\": Name, \"Error\": str(e)})\n",
    "            continue\n",
    "        try:\n",
    "            update_theme_permissions(targetsession, THEMEID, targetadmin)\n",
    "        except Exception as e:\n",
    "            #print('failed: '+str(e))\n",
    "            faillist.append({\"ThemeID\": THEMEID, \"Name\": Name, \"Error\": str(e)})\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print fail informations\n",
    "with open(faillocation+now+'Themes_Creation_Error.json', \"w\") as f:\n",
    "            json.dump(faillist, f, indent=4, sort_keys=True, default=str)\n",
    "\n",
    "successfulls=[]\n",
    "for news in newthemeslist:\n",
    "    theme=describe_theme(targetsession, news['ThemeId'])\n",
    "    successfulls.append(theme['Theme']['ThemeId'])\n",
    "    \n",
    "with open(successlocation+now+'Themes_Creation_Success.json', \"w\") as f:\n",
    "    json.dump(successfulls, f, indent=4, sort_keys=True, default=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sourceanalysis_list_complete=analysis(sourcesession)\n",
    "sourceanalysis_list=[]\n",
    "for a in sourceanalysis_list_complete:\n",
    "    if a[\"Name\"] in analysis_migrate_list:\n",
    "        sourceanalysis_list.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sourceanalysis_list_complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Migrate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sourceanalysis_all=[]\n",
    "for i in sourceanalysis_list:\n",
    "    if i['Status']!= 'DELETED':\n",
    "        sourceanalysis_all.append(i)\n",
    "\n",
    "success=[]\n",
    "faillist=[]\n",
    "sts_client = targetsession.client(\"sts\")\n",
    "account_id = sts_client.get_caller_identity()[\"Account\"]\n",
    "for i in sourceanalysis_all:\n",
    "    sourceanalysis=describe_analysis(sourcesession, i['AnalysisId'])\n",
    "    sourceanalysisid=sourceanalysis['Analysis']['AnalysisId']\n",
    "    sourceanalysisArn=sourceanalysis['Analysis']['Arn']\n",
    "    sourceanalysisname=sourceanalysis['Analysis']['Name']\n",
    "    DataSetArns=sourceanalysis['Analysis']['DataSetArns']\n",
    "    sourcetid=sourceanalysisid\n",
    "    sourcetname=sourceanalysisname\n",
    "    targettid=sourcetid\n",
    "    targettname=sourceanalysisname\n",
    "    \n",
    "    TargetThemeArn=''\n",
    "    if 'ThemeArn' in sourceanalysis['Analysis'].keys():\n",
    "        SourceThemeArn=sourceanalysis['Analysis']['ThemeArn']\n",
    "        TargetThemeArn = 'arn:aws:quicksight:'+region+':'+account_id+':theme/'+sourceanalysis['Analysis']['ThemeArn'].split(\"/\")[1]\n",
    "\n",
    "    sourcedsref = []\n",
    "    for i in DataSetArns:\n",
    "        missing=False\n",
    "        did = i.split(\"/\")[1]\n",
    "        try:\n",
    "            dname=get_dataset_name(did, sourcesession)\n",
    "        except Exception as e:\n",
    "            faillist.append({\"Error Type\": \"Dataset: \"+did+\" is missing!\",\"sourceanalysisid\": sourcetid, \"Name\": sourcetname, \"Error\": str(e)})\n",
    "            missing=True\n",
    "            break\n",
    "            \n",
    "        sourcedsref.append({'DataSetPlaceholder': dname,\n",
    "                    'DataSetArn': i})\n",
    "    if missing: continue\n",
    "    try:\n",
    "        sourcetemplate = create_template(sourcesession, sourcetid, sourcetname, sourcedsref, sourceanalysisArn, '1')\n",
    "        sourcetemplate=describe_template(sourcesession,sourcetid)\n",
    "    except Exception as e:\n",
    "        faillist.append({\"Error Type\": \"Create Source Template Error\",\"sourceanalysisid\": sourcetid, \"Name\": sourcetname, \"Error\": str(e)})\n",
    "\n",
    "        continue\n",
    "        \n",
    "    while sourcetemplate['Template']['Version']['Status']==\"CREATION_IN_PROGRESS\":\n",
    "        time.sleep(5)\n",
    "        sourcetemplate=describe_template(sourcesession,sourcetid)\n",
    "        if sourcetemplate['Template']['Version']['Status']==\"CREATION_SUCCESSFUL\":\n",
    "            try:\n",
    "                updateres=update_template_permission(sourcesession, sourcetid, targetroot)\n",
    "            except Exception as e:\n",
    "                delete_template(sourcesession, sourcetid)\n",
    "                faillist.append({\"Error Type\": \"Update Source Template Permission Error\",\n",
    "                                 \"sourceanalysisid\": sourcetid, \n",
    "                                 \"Name\": sourcetname, \n",
    "                                 \"Error\": str(e)})\n",
    "    else: \n",
    "        if sourcetemplate['Template']['Version']['Status']==\"CREATION_SUCCESSFUL\":\n",
    "            try:\n",
    "                updateres=update_template_permission(sourcesession, sourcetid, targetroot)\n",
    "            except Exception as e:\n",
    "                delete_template(sourcesession, sourcetid)\n",
    "                faillist.append({\"Error Type\": \"Update Source Template Permission Error\",\n",
    "                                 \"sourceanalysisid\": sourcetid, \n",
    "                                 \"Name\": sourcetname, \n",
    "                                 \"Error\": str(e)})\n",
    "                continue          \n",
    "\n",
    "    ds=data_sets (targetsession)\n",
    "    Template=sourcetemplate['Template']\n",
    "    dsref=[]\n",
    "    \n",
    "    missing=False\n",
    "    for i in Template['Version']['DataSetConfigurations']:\n",
    "    #print(i)\n",
    "        n=Template['Version']['DataSetConfigurations'].index(i)\n",
    "    #print(n)\n",
    "        for j in ds:\n",
    "            if i['Placeholder']==j['Name']:\n",
    "                dsref.append({\n",
    "                    'DataSetPlaceholder': i['Placeholder'],\n",
    "                    'DataSetArn': j['Arn']\n",
    "                })\n",
    "            if n>len(dsref): \n",
    "                e=\"Dataset \"+i['Placeholder']+\"is missing!\"\n",
    "                faillist.append({\"Error Type\": \"Datasets in target env are missing for this analysis\",\n",
    "                                 \"sourceanalysisid\": sourcetid, \n",
    "                                 \"Name\": sourcetname, \n",
    "                                 \"Error\": str(e)})\n",
    "                missing=True\n",
    "                break\n",
    "        if missing: break\n",
    "    if missing: continue\n",
    "        \n",
    "##working\n",
    "    SourceEntity={\n",
    "        'SourceTemplate': {\n",
    "            'DataSetReferences': dsref,\n",
    "            'Arn': Template['Arn']\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    #print(SourceEntity)\n",
    "    analysis=describe_analysis(targetsession, targettid)\n",
    "    if 'Faild to describe analysis:' in analysis or analysis['Analysis']['Status']=='DELETED':\n",
    "        if 'analysis/'+targettid+' is not found' in analysis or analysis['Analysis']['Status']=='DELETED':\n",
    "            print(\"Create new anlaysis now:\")\n",
    "            try:\n",
    "                newanalysis=create_analysis(targetsession, targettid, targettname,targetadmin,SourceEntity,TargetThemeArn)\n",
    "            except Exception as e:\n",
    "                delete_template(sourcesession, targettid)\n",
    "                faillist.append({\"Error Type\": \"Create New Analysis Error\",\n",
    "                                 \"AnalysisID\": targettid, \n",
    "                                 \"Name\": targettname, \n",
    "                                 \"Error\": str(e)}) \n",
    "                continue\n",
    "        else:\n",
    "            faillist.append({\"Error Type\": \"Describe Target Analysis Error\",\n",
    "                                 \"AnalysisID\": targettid, \n",
    "                                 \"Name\": targettname, \n",
    "                                 \"Error\": str(analysis)}) \n",
    "            continue\n",
    "    elif analysis['Analysis']['Status']==\"CREATION_FAILED\":\n",
    "        res=delete_analysis(sourcesession, targettid)\n",
    "        try:\n",
    "            newanalysis=create_analysis(targetsession, targettid, targettname,targetadmin, SourceEntity,TargetThemeArn)\n",
    "        except Exception as e:\n",
    "            delete_template(sourcesession, targettid)\n",
    "            faillist.append({\"Error Type\": \"Create Analysis Error\",\n",
    "                                 \"AnalysisID\": targettid, \n",
    "                                 \"Name\": targettname, \n",
    "                                 \"Error\": str(e)})\n",
    "            continue\n",
    "            \n",
    "    else:\n",
    "        print(\"analysis is existing. update it now.\")\n",
    "        try:\n",
    "            newanalysis=update_analysis(targetsession, targettid, targettname, SourceEntity,TargetThemeArn)\n",
    "        except Exception as e:\n",
    "            delete_template(sourcesession, targettid)\n",
    "            faillist.append({\"Error Type\": \"Update Analysis Error\",\n",
    "                                 \"AnalysisID\": targettid, \n",
    "                                 \"Name\": targettname, \n",
    "                                 \"Error\": str(e)})\n",
    "            continue\n",
    "    time.sleep(20)\n",
    "    res=describe_analysis(targetsession,newanalysis['AnalysisId'])\n",
    "    if res['Status']==200:\n",
    "        status=res['Analysis']['Status']\n",
    "        if status=='CREATION_SUCCESSFUL' or status=='UPDATE_SUCCESSFUL':\n",
    "            success.append(res['Analysis'])\n",
    "            #filename=\"Migration_Results/Successful/Analysis_\"+res['Analysis']['Name']+\".json\"\n",
    "        else:\n",
    "            faillist.append({\"Error Type\": \"Analysis Creation Status is not Successful\", \"Analysis\": res['Analysis']})\n",
    "            #filename=\"Migration_Results/Fail/Analysis_\"+res['Analysis']['Name']+\".json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(faillocation+now+'Analysis_Error.json', \"w\") as f:\n",
    "            json.dump(faillist, f, indent=4, sort_keys=True, default=str)\n",
    "\n",
    "with open(successlocation+now+'Analysis_Success.json', \"w\") as f:\n",
    "    json.dump(success, f, indent=4, sort_keys=True, default=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Get dashboards list</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#source account\n",
    "sourceaccountid=\"\"\n",
    "role_name=\"\"\n",
    "aws_region='us-east-1'\n",
    "sourcesession = _assume_role(sourceaccountid, role_name, aws_region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#target account\n",
    "targetaccountid=\"\"\n",
    "role_name=\"\"\n",
    "aws_region='us-east-1'\n",
    "targetsession = _assume_role(targetaccountid, role_name, aws_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sourcedashboards=dashboards(sourcesession)\n",
    "\n",
    "#Get id of datasets migration list\n",
    "migration_list=[]\n",
    "for newset in dashboard_migrate_list:\n",
    "        ids = get_dashboard_ids(newset, sourcesession)\n",
    "        for dashboard in sourcedashboards:\n",
    "            if ids[0] == dashboard[\"DashboardId\"]:\n",
    "                migration_list.append(dashboard)\n",
    "\n",
    "JSON(migration_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Migrate dashboards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success=[]\n",
    "faillist=[]\n",
    "for dashboard in migration_list:\n",
    "    sourcedashboard=describe_dashboard(sourcesession, dashboard['DashboardId'])\n",
    "    SourceEntityArn=sourcedashboard['Dashboard']['Version']['SourceEntityArn']\n",
    "    print(SourceEntityArn)\n",
    "    if SourceEntityArn.split(\"/\")[0].split(\":\")[-1]==\"analysis\" or SourceEntityArn.split(\"/\")[0].split(\":\")[-1]=='template':\n",
    "        sourceanalysis=sourcedashboard['Dashboard']['Version']['SourceEntityArn']\n",
    "    else: \n",
    "        faillist.append({\"Error Type\": \"Source Analysis is missing!\",\"DashboardId\": sourcetid, \"Name\": sourcetname, \"Error\": \"Source Analysis is missing!\"})\n",
    "        continue\n",
    "    \n",
    "    sourceversion=sourcedashboard['Dashboard']['Version']['VersionNumber']\n",
    "    sourcedid=sourcedashboard['Dashboard']['DashboardId']\n",
    "    sourcedname=sourcedashboard['Dashboard']['Name']\n",
    "    sourcetid=sourcedid\n",
    "    sourcetname=sourcedname\n",
    "    targetdid=get_target_id(sourcetid)\n",
    "    targetdname=get_target_id(sourcetname)\n",
    "    print('1')\n",
    "    \n",
    "        \n",
    "    DataSetArns=sourcedashboard['Dashboard']['Version']['DataSetArns']\n",
    "    TargetThemeArn=''\n",
    "    if 'ThemeArn' in sourcedashboard['Dashboard']['Version'].keys():\n",
    "        SourceThemearn=sourcedashboard['Dashboard']['Version']['ThemeArn']\n",
    "        TargetThemeArn = 'arn:aws:quicksight:'+region+':'+account_id+':theme/'+SourceThemearn.split(\"/\")[1]\n",
    "    sourcedsref = []\n",
    "    print('2')\n",
    "    for i in DataSetArns:\n",
    "        missing=False\n",
    "        did = i.split(\"/\")[1]\n",
    "        try:\n",
    "            dname=get_dataset_name(did, sourcesession)\n",
    "        except Exception as e:\n",
    "            faillist.append({\"Error Type\": \"Dataset: \"+did+\" is missing!\",\"DashboardId\": sourcetid, \"Name\": sourcetname, \"Error\": str(e)})\n",
    "            missing=True\n",
    "            break\n",
    "\n",
    "        sourcedsref.append({'DataSetPlaceholder': dname,\n",
    "                        'DataSetArn': i})\n",
    "    if missing: continue\n",
    "            \n",
    "    if SourceEntityArn.split(\"/\")[0].split(\":\")[-1]==\"analysis\" and same_account_migration == True:\n",
    "\n",
    "        try:\n",
    "            sourcetemplate = create_template(sourcesession, sourcetid, sourcetname, sourcedsref, sourceanalysis, '1')\n",
    "            sourcetemplate = describe_template(sourcesession,sourcetid)\n",
    "            print('template created')\n",
    "        except Exception as e:\n",
    "            faillist.append({\"Error Type\": \"Create Source Template Error\",\"DashboardId\": sourcetid, \"Name\": sourcetname, \"Error\": str(e)})\n",
    "            continue\n",
    "        print('3')\n",
    "        while sourcetemplate['Template']['Version']['Status']==\"CREATION_IN_PROGRESS\":\n",
    "            time.sleep(5)\n",
    "            sourcetemplate=describe_template(sourcesession,sourcetid)\n",
    "            #print(sourcetemplate)\n",
    "\n",
    "        print('4')\n",
    "        sourcetemplate = describe_template(sourcesession,sourcetid)\n",
    "        targettemplate = describe_template(targetsession,sourcetid) #there is no need to copy template within the same account\n",
    "\n",
    "        while targettemplate['Template']['Version']['Status']==\"CREATION_IN_PROGRESS\":\n",
    "            time.sleep(5)\n",
    "            targettemplate=describe_template(targetsession,sourcetid)\n",
    "            if targettemplate['Template']['Version']['Status']==\"CREATION_SUCCESSFUL\":\n",
    "                break\n",
    "        else: \n",
    "            if targettemplate['Template']['Version']['Status']==\"CREATION_SUCCESSFUL\":\n",
    "                print(\"Template is successful copied!\")\n",
    "            else: \n",
    "                delete_template(targetsession, sourcetid)\n",
    "                faillist.append({\"Error Type\": \"Copy Template Error\",\n",
    "                                     \"DashboardId\": sourcetid, \n",
    "                                     \"Name\": sourcetname, \n",
    "                                     \"Error\": str(e)})\n",
    "                continue\n",
    "\n",
    "    elif SourceEntityArn.split(\"/\")[0].split(\":\")[-1]==\"template\" and same_account_migration == True:\n",
    "        sourcetid = SourceEntityArn.split(\"/\")[1]\n",
    "        sourcetemplate = describe_template(sourcesession,sourcetid)\n",
    "        targettemplate = describe_template(targetsession,sourcetid)\n",
    "        \n",
    "    elif SourceEntityArn.split(\"/\")[0].split(\":\")[-1]==\"analysis\" and same_account_migration == False:\n",
    "        #To fill\n",
    "        print('to do')\n",
    "            \n",
    "    elif SourceEntityArn.split(\"/\")[0].split(\":\")[-1]==\"analysis\" and same_account_migration == False:\n",
    "        #To fill\n",
    "        print('to do')\n",
    "        \n",
    "        \n",
    "    #ds=data_sets (targetsession)\n",
    "    ds = folder_members (targetsession, target_folder_ID)\n",
    "    Template=targettemplate['Template']\n",
    "    dsref=[]\n",
    "    #print(Template['Version']['DataSetConfigurations'])\n",
    "    missing=False\n",
    "    for i in Template['Version']['DataSetConfigurations']:\n",
    "        #print(\"i is \"+str(i))\n",
    "        n=Template['Version']['DataSetConfigurations'].index(i)\n",
    "        #print(\"n is \"+str(n))\n",
    "        for j in ds:\n",
    "            member_type = j['MemberArn'].split('/')[0].split(':')[-1]\n",
    "            if member_type != 'dataset':\n",
    "                continue\n",
    "            j['Arn'] = j['MemberArn']    \n",
    "            ds_des = describe_dataset (targetsession, j['MemberId'])\n",
    "            j['Name'] = ds_des['DataSet']['Name']\n",
    "            if get_target_placeholder(i['Placeholder'])==j['Name']:\n",
    "                print(i['Placeholder'])\n",
    "                print(j['Name'])\n",
    "                print(j['Arn'])\n",
    "                dsref.append({\n",
    "                    'DataSetPlaceholder': i['Placeholder'],\n",
    "                    'DataSetArn': j['Arn']\n",
    "                })\n",
    "                break\n",
    "                print(\"len of dsref is \"+str(len(dsref)))\n",
    "                print(dsref)\n",
    "        if (n+1)>len(dsref): \n",
    "            e=\"Dataset \"+i['Placeholder']+\" is missing!\"\n",
    "            faillist.append({\"Error Type\": \"Datasets in target env are missing for this dashboard\",\n",
    "                                 \"DashboardId\": sourcetid, \n",
    "                                 \"Name\": sourcetname, \n",
    "                                 \"Error\": str(e)})\n",
    "            missing=True\n",
    "            break\n",
    "        if missing: break\n",
    "    if missing: continue\n",
    "    #print(\"len of dsref is \"+str(len(dsref)))\n",
    "    #print(dsref) \n",
    "    \n",
    "    if target_is_dev == True: # create an analysis for dev purposes first\n",
    "        print('create an analysis for dev purposes first')\n",
    "        SourceEntity={\n",
    "            'SourceTemplate': {\n",
    "                'DataSetReferences': dsref,\n",
    "                'Arn': Template['Arn']\n",
    "            }\n",
    "        }\n",
    "        newanalysis=create_analysis(targetsession, targetdid, targetdname, targetadmin, SourceEntity,TargetThemeArn)\n",
    "        #print(newanalysis)\n",
    "        time.sleep(30)\n",
    "        create_folder_membership(targetsession, target_folder_ID, targetdid, 'ANALYSIS' )\n",
    "        continue\n",
    "        \n",
    "    else:    \n",
    "        SourceEntity={\n",
    "            'SourceTemplate': {\n",
    "                'DataSetReferences': dsref,\n",
    "                'Arn': Template['Arn']\n",
    "            }\n",
    "        }\n",
    "    #print(SourceEntity)\n",
    "    dashboard=describe_dashboard(targetsession, targetdid)\n",
    "\n",
    "    if 'Faild to describe dashboard:' in dashboard:\n",
    "        if 'dashboard/'+targetdid+' is not found' in dashboard:\n",
    "            print(\"Create new dashboard now:\")\n",
    "            try:\n",
    "                newdashboard=create_dashboard(targetsession, targetdid, targetdname,targetadmin, SourceEntity, '1',TargetThemeArn, filter='DISABLED',csv='ENABLED', sheetcontrol='COLLAPSED')\n",
    "            except Exception as e:\n",
    "                delete_template(targetsession, targetdid)\n",
    "                faillist.append({\"Error Type\": \"Create New Dashboard Error\",\n",
    "                                 \"DashboardId\": targetdid, \n",
    "                                 \"Name\": targetdname, \n",
    "                                 \"Error\": str(e)}) \n",
    "                continue\n",
    "        else: \n",
    "            faillist.append({\"Error Type\": \"Describe Target Dashboard Error\",\n",
    "                                 \"DashboardId\": targetdid, \n",
    "                                 \"Name\": targetdname, \n",
    "                                 \"Error\": str(dashboard)}) \n",
    "            continue\n",
    "    elif dashboard['Dashboard']['Version']['Status']==\"CREATION_FAILED\":\n",
    "        res=delete_dashboard(targetsession, targetdid)\n",
    "        try:\n",
    "            newdashboard=create_dashboard(targetsession, targetdid, targetdname,targetadmin, SourceEntity, '1',TargetThemeArn, filter='DISABLED',csv='ENABLED', sheetcontrol='COLLAPSED')\n",
    "        except Exception as e:\n",
    "            delete_template(targetsession, targetdid)\n",
    "            print('fail to create dashboard, add to faillist')\n",
    "            faillist.append({\"Error Type\": \"Create Dashboard Error\",\n",
    "                                 \"DashboardId\": targetdid, \n",
    "                                 \"Name\": targetdname, \n",
    "                                 \"Error\": str(e)})\n",
    "            continue\n",
    "            \n",
    "    else:\n",
    "        print(\"dashboard is existing. update it now.\")\n",
    "        try:\n",
    "            res=delete_dashboard(targetsession, targetdid)\n",
    "            newdashboard=create_dashboard(targetsession, targetdid, targetdname,targetadmin, SourceEntity, '1',TargetThemeArn, filter='DISABLED',csv='ENABLED', sheetcontrol='COLLAPSED')\n",
    "        except Exception as e:\n",
    "            #print(newdashboard)\n",
    "            delete_template(targetsession, targetdid)\n",
    "            print('fail to update dashboard, add to faillist')\n",
    "            faillist.append({\"Error Type\": \"Create Dashboard Error\",\n",
    "                                 \"DashboardId\": targetdid, \n",
    "                                 \"Name\": targetdname, \n",
    "                                 \"Error\": str(e)})\n",
    "            continue\n",
    "    while(True):\n",
    "        res=describe_dashboard(targetsession,newdashboard['DashboardId'])\n",
    "\n",
    "        if res['Status']==200:\n",
    "            status=res['Dashboard']['Version']['Status']\n",
    "            if status=='CREATION_SUCCESSFUL' or status=='UPDATE_SUCCESSFUL':\n",
    "                success.append(res['Dashboard'])\n",
    "                break\n",
    "            elif status=='CREATION_IN_PROGRESS':\n",
    "                time.sleep(10)\n",
    "                continue\n",
    "            else:\n",
    "                faillist.append({\"Error Type\": \"Dashboard Creation Status is not Successful\", \"Dashboard\": res['Dashboard']})\n",
    "                break\n",
    "                #filename=\"Migration_Results/Fail/Dashboard_\"+res['Dashboard']['Name']+\".json\"\n",
    "            \n",
    "    create_folder_membership(targetsession, target_folder_ID, targetdid, 'DASHBOARD' )\n",
    "    print('added dashboard to {} folder'.format(target_folder_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(faillocation+now+'Dashboard_Error.json', \"w\") as f:\n",
    "            json.dump(faillist, f, indent=4, sort_keys=True, default=str)\n",
    "\n",
    "with open(successlocation+now+'Dashboard_Success.json', \"w\") as f:\n",
    "    json.dump(success, f, indent=4, sort_keys=True, default=str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faillist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Delete objects<strong/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THIS WILL DELETE ALL TARGET DATASETS\n",
    "\n",
    "delete = \"template\"\n",
    "\n",
    "if delete == \"datasource\":   \n",
    "    for datasource in data_sources(targetsession):\n",
    "        #if datasource['Type'] == \"REDSHIFT\":\n",
    "        try:\n",
    "            delete_source (targetsession, datasource['DataSourceId'])\n",
    "        except Exception: pass \n",
    "elif delete == \"dataset\":\n",
    "    for dataset in data_sets(targetsession):\n",
    "        delete_dataset (targetsession, dataset['DataSetId'])\n",
    "elif delete == \"template\":    \n",
    "    for template in templates(targetsession):\n",
    "        delete_template(targetsession, template['TemplateId'])\n",
    "elif delete == \"analysis\":\n",
    "    for analysis in analysis(targetsession): delete_analysis(targetsession, analysis['AnalysisId'])\n",
    "    \n",
    "elif delete == \"dashboard\":    \n",
    "    for dashboard in dashboards(targetsession):\n",
    "        delete_dashboard(targetsession, dashboard['DashboardId'])\n",
    "delete =\"don't delete anything\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schedule notebooks to execute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://aws.amazon.com/blogs/machine-learning/scheduling-jupyter-notebooks-on-sagemaker-ephemeral-instances/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc-showcode": false,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
